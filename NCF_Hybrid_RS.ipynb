{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MIE1624 - RS Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLfjdg-syXWp"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcPv0cRo7H2u",
        "outputId": "c8d77d9a-eed0-478c-a135-3f2c50b8d4c2"
      },
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "from tqdm import trange\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "import csv\n",
        "## Download Resources\n",
        "nltk.download(\"vader_lexicon\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "from nltk import tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import PerceptronTagger\n",
        "from nltk.data import find\n",
        "# Processing\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics.pairwise import pairwise_distances\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Keras\n",
        "from keras.layers import Dense, Dropout, Flatten, Layer\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
        "from keras.layers import Embedding, Input, dot, concatenate, merge\n",
        "from keras.models import Model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibq5E47ehJne"
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "6cW3kZnhyRLO",
        "outputId": "4e509984-103d-4908-af11-5730660214db"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "      <th>category</th>\n",
              "      <th>price</th>\n",
              "      <th>itemID</th>\n",
              "      <th>reviewHash</th>\n",
              "      <th>image</th>\n",
              "      <th>overall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>08 26, 2014</td>\n",
              "      <td>u92990698</td>\n",
              "      <td>A contemporary jazz and soul performer who's m...</td>\n",
              "      <td>\" CLASSIQUE \"</td>\n",
              "      <td>1409011200</td>\n",
              "      <td>Jazz</td>\n",
              "      <td>$8.40</td>\n",
              "      <td>p23649501</td>\n",
              "      <td>3856620</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>05 11, 2003</td>\n",
              "      <td>u36200649</td>\n",
              "      <td>Very good idea to put both the 'pop' and 'orch...</td>\n",
              "      <td>Exceeded my Expectations - This album RAWKS!</td>\n",
              "      <td>1052611200</td>\n",
              "      <td>Alternative Rock</td>\n",
              "      <td>$10.98</td>\n",
              "      <td>p58458313</td>\n",
              "      <td>56086781</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12 5, 2017</td>\n",
              "      <td>u10721702</td>\n",
              "      <td>This is a great collection of Carole King's so...</td>\n",
              "      <td>A Must-have for Carole King Fans</td>\n",
              "      <td>1512432000</td>\n",
              "      <td>Pop</td>\n",
              "      <td>$5.99</td>\n",
              "      <td>p97027626</td>\n",
              "      <td>55852154</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>09 24, 2013</td>\n",
              "      <td>u86003775</td>\n",
              "      <td>The is album is a brilliant piece of Jazz fusi...</td>\n",
              "      <td>A Master piece!</td>\n",
              "      <td>1379980800</td>\n",
              "      <td>Jazz</td>\n",
              "      <td>$14.64</td>\n",
              "      <td>p43167086</td>\n",
              "      <td>43228100</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>01 14, 2015</td>\n",
              "      <td>u25672859</td>\n",
              "      <td>Waited a LONG time for this DVD to be released...</td>\n",
              "      <td>especially if you like concert videos</td>\n",
              "      <td>1421193600</td>\n",
              "      <td>Alternative Rock</td>\n",
              "      <td>$9.92</td>\n",
              "      <td>p94494236</td>\n",
              "      <td>54425467</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    reviewTime reviewerID  ... image overall\n",
              "0  08 26, 2014  u92990698  ...   NaN       5\n",
              "1  05 11, 2003  u36200649  ...   NaN       5\n",
              "2   12 5, 2017  u10721702  ...   NaN       5\n",
              "3  09 24, 2013  u86003775  ...   NaN       5\n",
              "4  01 14, 2015  u25672859  ...   NaN       5\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr8zkksbQSOa"
      },
      "source": [
        "# Possibly add time dimension by year?  Slight uptick in rating over time\n",
        "df['year'] = df['reviewTime'].str[-4:]\n",
        "df.groupby('year')['overall'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqWR3mKByZLY"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v7Vc97ej83c"
      },
      "source": [
        "class Pipeline():\n",
        "    def __init__(self,df):\n",
        "        self.df = df\n",
        "\n",
        "    def encode(self, users=\"reviewerID\", items=\"itemID\"):\n",
        "        self.df['user'] = LabelEncoder().fit_transform(self.df[users])\n",
        "        self.df['item'] = LabelEncoder().fit_transform(self.df[items])        \n",
        "    \n",
        "    def create_matrix(self, users=\"reviewerID\", items=\"itemID\", ratings=\"overall\"):\n",
        "        self.num_users = self.df[users].nunique()\n",
        "        self.num_items = self.df[items].nunique()\n",
        "        self.matrix = np.zeros((self.num_users, self.num_items), dtype=np.int8)\n",
        "        \n",
        "        for (index, userID, itemID, rating) in self.df[['user','item',ratings]].itertuples():\n",
        "            self.matrix[userID-1, itemID-1] = rating\n",
        "\n",
        "    def get_stop_words(self):\n",
        "        self.stop = set(stopwords.words('english'))\n",
        "    \n",
        "    def getTopKWords(self,kwords,column):\n",
        "        self.get_stop_words()\n",
        "        self.counter = Counter()\n",
        "        text = self.df[column].values\n",
        "        for t in text:\n",
        "                self.counter.update(t)\n",
        "        self.topk = self.counter.most_common(kwords)\n",
        "\n",
        "    def tokenize(self,column):\n",
        "        self.df[column] = self.df[column].fillna(\"\")\n",
        "        self.df[column+'_tokens'] = self.df[column].apply(lambda x:[token.lower() for token in re.findall(r'\\w+',x) if token.lower() not in self.stop and len(token) > 2])\n",
        "\n",
        "    def datetime(self,column='reviewTime'):\n",
        "        self.df['datetime'] = pd.to_datetime(df[column])\n",
        "        \n",
        "    def price(self,column='price'):\n",
        "        self.df.loc[self.df[column].str.len()>10,column] = 0\n",
        "        self.df[column] = self.df[column].str.replace(\"$\",\"\").astype(float)\n",
        "\n",
        "    def binary_score(self,column=\"overall\",positive=[4,5]):\n",
        "        self.df.loc[self.df[column].isin(positive),\"binary\"] = 1\n",
        "        self.df.loc[~self.df[column].isin(positive),\"binary\"] = 0\n",
        "\n",
        "    def n_reviews(self):\n",
        "        self.df = self.df.merge(self.df.groupby(by='user')['item'].count().reset_index().rename(columns={'item':'n_reviews'}),how='left',on=['user'])\n",
        "\n",
        "    def run(self,date=False):\n",
        "        #self.create_matrix()\n",
        "        self.encode()\n",
        "        self.get_stop_words()\n",
        "        self.tokenize(column=\"reviewText\")\n",
        "        self.tokenize(column=\"summary\")\n",
        "        self.tokenize(column=\"category\")\n",
        "        self.df['tokens'] = self.df['reviewText_tokens'] + self.df['summary_tokens'] + self.df['category_tokens']\n",
        "        self.datetime()\n",
        "        self.price()\n",
        "        #self.binary_score()\n",
        "        self.n_reviews()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYcrLg00nEBJ"
      },
      "source": [
        "pipeline = Pipeline(df)\n",
        "pipeline.run()\n",
        "df = pipeline.df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7waDx-Us-ioU"
      },
      "source": [
        "# We need to define a custom train-test split function for handling dataframes\n",
        "# Pandas series sampling can handle this\n",
        "# Ensure we have 1 sample per user\n",
        "def tt_split(df,train_size=0.8,random_state=1,cols=['reviewerID','itemID','overall','summary','reviewText']):\n",
        "    # Need to have 1 of each item, create item set\n",
        "    unique_items = set(df.item.unique())\n",
        "    \n",
        "    # Sample 1 row per user from shuffled df and store index\n",
        "    df_users = df.sample(frac=1).drop_duplicates(subset='user')\n",
        "    df_users_idx = df_users.index\n",
        "\n",
        "    # Filter out sampled rows\n",
        "    df1 = df.loc[~df.index.isin(df_users_idx)].copy()\n",
        "    \n",
        "    # Find out which items we sampled and which we still require\n",
        "    sampled_items = set(df_users.item.unique())\n",
        "    required_items = unique_items - sampled_items\n",
        "    \n",
        "    # In filtered df now sample so we have 1 of each item\n",
        "    df_items = df1.loc[df1.item.isin(required_items)].sample(frac=1).drop_duplicates(subset='item')\n",
        "    df_items_idx = df_items.index\n",
        "    \n",
        "    # Find remaining number of samples to meet train-test split ratio\n",
        "    n_samples = round(train_size*df.shape[0] - df_users.shape[0] - df_items.shape[0])\n",
        "    base = df_users.append(df_items)\n",
        "    base_idx = df_users_idx.append(df_items_idx)\n",
        "    df2 = df1.loc[~df1.index.isin(base_idx)].copy()\n",
        "    \n",
        "    train = base.append(df2.sample(n_samples))\n",
        "    train_idx = train.index\n",
        "    train = train.reset_index(drop=True)\n",
        "    test = df2.loc[~df2.index.isin(train_idx)].copy()\n",
        "    \n",
        "    test_idx = test.index\n",
        "    test = test.reset_index(drop=True)\n",
        "    return train,test,train_idx,test_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ppwihUG_7-U"
      },
      "source": [
        "train,val,train_idx,val_idx = tt_split(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-6rLBJzP882"
      },
      "source": [
        "train_rating_df = train[['user','item','overall']].copy()\n",
        "num_user = train_rating_df.user.nunique()\n",
        "num_item = train_rating_df.item.nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B4V-Gquya5f"
      },
      "source": [
        "# PMF Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ5ZsTsSyUJp"
      },
      "source": [
        "class NeuralPMF(object):\n",
        "    \"\"\"\n",
        "    You can define new methods if you need. Don't use global variables in the class. \n",
        "    \"\"\"\n",
        "    def __init__(self, num_feat=5, epsilon=1,_lambda=0.1,momentum=0.6,maxepoch=12,num_batches=32,batch_size=1000,optimizer = Adam(lr=0.002),loss=\"mean_squared_error\"):\n",
        "        \"\"\"\n",
        "        Initialization of the class\n",
        "        1. Make sure to fill out self.pred_column_name, the name you give to your competition method\n",
        "        num_feat: int, number of latent features\n",
        "        epsilon: float, learning rate\n",
        "        _lambda: float, L2 regularization,\n",
        "        momentum: float, momentum of the gradient,\n",
        "        maxepoch: float, Number of epoch before stop,\n",
        "        num_batches: int, Number of batches in each epoch (for SGD optimization),\n",
        "        batch_size: Number int, of training samples used in each batches (for SGD optimization)\n",
        "        \"\"\"\n",
        "        self.num_feat = num_feat  # Number of latent features,\n",
        "        self.epsilon = epsilon  # learning rate,\n",
        "        self._lambda = _lambda  # L2 regularization,\n",
        "        self.momentum = momentum  # momentum of the gradient,\n",
        "        self.maxepoch = maxepoch  # Number of epoch before stop,\n",
        "        self.num_batches = num_batches  # Number of batches in each epoch (for SGD optimization),\n",
        "        self.batch_size = batch_size  # Number of training samples used in each batches (for SGD optimization)\n",
        "        self.optimizer = optimizer\n",
        "        self.loss = loss\n",
        "        self.test = False\n",
        "        self.w_Item = None  # Item feature vectors\n",
        "        self.w_User = None  # User feature vectors\n",
        "        self.user_embeddings = None\n",
        "        self.item_embeddings = None\n",
        "        self.merged_w = None # Merged flattened User and Item vectors\n",
        "        self.dense_layer1 = None\n",
        "        self.batchnorm_layer1 = None\n",
        "        self.dense_layer2 = None\n",
        "        self.result = None\n",
        "        self.model = None\n",
        "        self.output = None\n",
        "        self.score = None\n",
        "        self.user_input = None\n",
        "        self.item_input = None\n",
        "        self.rmse_train = []\n",
        "        self.rmse_test = []\n",
        "        self.pred_column_name='RNN'\n",
        "\n",
        "    def predict_all(self, train_vec, num_user, num_item):\n",
        "        \"\"\"\n",
        "        INPUT: \n",
        "            data: pandas DataFrame. columns=['userID', 'itemID', 'rating'...]\n",
        "            num_user: scalar. number of users\n",
        "            num_item: scalar. number of items\n",
        "        OUTPUT:\n",
        "            no return... \n",
        "        \n",
        "        NOTES:\n",
        "            This function is where you train your model\n",
        "        \"\"\"\n",
        "        \n",
        "        num_user += 1\n",
        "        num_item += 1\n",
        "        train_vec = train_vec.iloc[:, :3].values\n",
        "        #train_vec, val_vec = train_test_split(train_vec)\n",
        "        self.user_input = Input(shape=(1,), name='User_Input')\n",
        "        self.user_embeddings = Embedding(input_dim=num_user, output_dim=self.num_feat, input_length=1, name='User_Embedding')(self.user_input)\n",
        "        self.w_User = Flatten(name='User_Vector')(self.user_embeddings)\n",
        "\n",
        "        self.item_input = Input(shape=(1,), name='Item_Input')\n",
        "        self.item_embeddings = Embedding(input_dim=num_item, output_dim=self.num_feat, input_length=1, name='Item_Embedding')(self.item_input)\n",
        "        self.w_Item = Flatten(name='Item_Vector')(self.item_embeddings)\n",
        "\n",
        "        self.merged_w = concatenate([self.w_User, self.w_Item], name='Merged_Vector')\n",
        "        self.dense_layer1 = Dropout(0.8)(Dense(10, activation=\"relu\")(self.merged_w))\n",
        "        self.batchnorm_layer1 = BatchNormalization()(self.dense_layer1)\n",
        "        #self.dense_layer2 = Dense(100,activation='relu')(self.batchnorm_layer1)\n",
        "        self.result = Dense(1)(self.dense_layer1)\n",
        "        self.model = Model([self.user_input,self.item_input],self.result)\n",
        "        self.model.compile(loss=self.loss,optimizer=self.optimizer)\n",
        "        self.output = self.model.fit(x=[train_vec[:,0],train_vec[:,1]],y=train_vec[:,2],batch_size=self.num_batches,epochs=self.maxepoch)\n",
        "        #self.output.save(\"rs_model_v1\")\n",
        "\n",
        "    def evaluate_test(self, test_df, copy=False):\n",
        "        \"\"\"\n",
        "            INPUT:\n",
        "                data: pandas DataFrame. columns=['userID', 'itemID', 'rating'...]\n",
        "            OUTPUT:\n",
        "                predictions:  pandas DataFrame. \n",
        "                              columns=['userID', 'itemID', 'rating', 'base-method'...]\n",
        "\n",
        "            NOTES:\n",
        "            This function is where your model makes prediction \n",
        "            Please fill out: prediction.loc[index, self.pred_column_name] = None                            \n",
        "                              \n",
        "        \"\"\"\n",
        "        if copy:\n",
        "            prediction = pd.DataFrame(test_df.copy(), columns=['userID', 'itemID', 'rating'])\n",
        "        else:\n",
        "            prediction = pd.DataFrame(test_df, columns=['userID', 'itemID', 'rating'])\n",
        "        prediction[self.pred_column_name] = np.nan\n",
        "        \n",
        "        for (index, \n",
        "             userID, \n",
        "             itemID) in tqdm(prediction[['userID','itemID']].itertuples()):\n",
        "            prediction.loc[index, self.pred_column_name] = self.model.predict([np.array([int(userID)]),np.array([int(itemID)])])[0][0]\n",
        "        return prediction\n",
        "          \n",
        "    def getPredColName(self):\n",
        "        \"\"\"\n",
        "            return prediction column name\n",
        "        \"\"\"\n",
        "        return self.pred_column_name\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "            reuse the instance of the class by removing model\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.w_Item = None  # Item feature vectors\n",
        "            self.w_User = None  # User feature vectors\n",
        "            self.user_embeddings = None\n",
        "            self.item_embeddings = None\n",
        "            self.merged_w = None # Merged flattened User and Item vectors\n",
        "            self.dense_layer1 = None\n",
        "            self.batchnorm_layer1 = None\n",
        "            self.dense_layer2 = None\n",
        "            self.result = None\n",
        "            self.model = None\n",
        "            self.output = None\n",
        "            self.score = None\n",
        "            self.user_input = None\n",
        "            self.item_input = None\n",
        "        except:\n",
        "            print(\"You do not have w_Item, w_User\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b8CxZLyzNVh"
      },
      "source": [
        "# Run PMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZS7jW7uqay0"
      },
      "source": [
        "import keras\n",
        "from google.colab import files\n",
        "load_keras_model = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "5ny-SH8FqXfC",
        "outputId": "f9886ce9-3539-4fd1-a1c6-3f91a0a483a3"
      },
      "source": [
        "if load_keras_model == True:\n",
        "    !unzip rs_model_v2.zip\n",
        "    rs_model = keras.models.load_model(\"rs_model_v2\")\n",
        "else:\n",
        "    rs = NeuralPMF()#pmf_recsys.PMFRecSys()\n",
        "    rs.predict_all(train_rating_df,num_user,num_item)\n",
        "    rs.model.save(\"rs_model_v5\")\n",
        "    !zip -r rs_model_v5.zip rs_model_v5\n",
        "    files.download(\"rs_model_v5.zip\")\n",
        "    rs_model = keras.models.load_model(\"rs_model_v5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "3750/3750 [==============================] - 45s 12ms/step - loss: 7.5837\n",
            "Epoch 2/12\n",
            "3750/3750 [==============================] - 41s 11ms/step - loss: 0.9403\n",
            "Epoch 3/12\n",
            "3750/3750 [==============================] - 40s 11ms/step - loss: 0.8474\n",
            "Epoch 4/12\n",
            "3750/3750 [==============================] - 41s 11ms/step - loss: 0.8245\n",
            "Epoch 5/12\n",
            "3750/3750 [==============================] - 42s 11ms/step - loss: 0.8010\n",
            "Epoch 6/12\n",
            "3750/3750 [==============================] - 39s 11ms/step - loss: 0.7810\n",
            "Epoch 7/12\n",
            "3750/3750 [==============================] - 39s 10ms/step - loss: 0.7319\n",
            "Epoch 8/12\n",
            "3750/3750 [==============================] - 42s 11ms/step - loss: 0.7262\n",
            "Epoch 9/12\n",
            "3750/3750 [==============================] - 40s 11ms/step - loss: 0.7111\n",
            "Epoch 10/12\n",
            "3750/3750 [==============================] - 40s 11ms/step - loss: 0.7024\n",
            "Epoch 11/12\n",
            "3750/3750 [==============================] - 39s 10ms/step - loss: 0.6859\n",
            "Epoch 12/12\n",
            "3750/3750 [==============================] - 39s 10ms/step - loss: 0.6557\n",
            "INFO:tensorflow:Assets written to: rs_model_v5/assets\n",
            "  adding: rs_model_v5/ (stored 0%)\n",
            "  adding: rs_model_v5/assets/ (stored 0%)\n",
            "  adding: rs_model_v5/variables/ (stored 0%)\n",
            "  adding: rs_model_v5/variables/variables.data-00000-of-00001 (deflated 27%)\n",
            "  adding: rs_model_v5/variables/variables.index (deflated 61%)\n",
            "  adding: rs_model_v5/saved_model.pb (deflated 88%)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6709d52b-d33f-4019-8329-151ad62bb4c8\", \"rs_model_v5.zip\", 3852353)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5uwpGveRnx8",
        "outputId": "c9f2e5e2-13c7-49d2-8b1e-c74d894b8701"
      },
      "source": [
        "val_rating_df = val[['user','item','overall']].copy()\n",
        "val_rating_df['RNN'] = rs_model.predict([val_rating_df.user.values,val_rating_df.item.values])\n",
        "train_rating_df['RNN'] = rs_model.predict([train_rating_df.user.values,train_rating_df.item.values])\n",
        "print(np.mean((val_rating_df['overall'] - val_rating_df['RNN'])**2))\n",
        "print(np.mean((train_rating_df['overall'] - train_rating_df['RNN'])**2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.862684428691864\n",
            "0.483529657125473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "rb2iAJWQ1BTN",
        "outputId": "5ee88f12-dc60-44d8-90b6-1e591dbad9d5"
      },
      "source": [
        "# Evaluate RMSE by Number of Reviews\n",
        "val = val.merge(val_rating_df,how='left',on=['user','item','overall'])\n",
        "val['SE'] = (val['RNN'] - val['overall'])**2\n",
        "r1 = val.groupby(by='n_reviews')[['SE']].mean().reset_index()\n",
        "r1['RMSE'] = np.sqrt(r1['SE'])\n",
        "plt.scatter(r1.n_reviews,r1.RMSE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f85fdd6f590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQNBhlEZW3R2"
      },
      "source": [
        "# Summary Flag Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "souNzKSEW7ET"
      },
      "source": [
        "def star_flags(dataframe):\n",
        "    df = dataframe.copy()\n",
        "    \n",
        "    # Create Flags\n",
        "    df['star_flag'] = df['summary_tokens'].apply(lambda x: int('star' in x or 'stars' in x))\n",
        "    for n in [(1,'one'),(2,'two'),(3,'three'),(4,'four'),(5,'five')]:\n",
        "        df['num_'+str(n[0])+'_flag'] = df['summary'].apply(lambda x: int(str(n[0])+' ' in x))\n",
        "        df['word_'+str(n[0])+'_flag'] = df['summary_tokens'].apply(lambda x: int(n[1] in x))\n",
        "        df['num_'+str(n[0])+'pt_flag'] = 0\n",
        "        df.loc[df.summary.str.contains(\"[\"+str(n[0])+\"]{1}([.][5]{1})\"),'num_'+str(n[0])+'pt_flag'] = 1\n",
        "    \n",
        "    # Make Predictions\n",
        "    predictions = [[],[],[],[],[]]\n",
        "    for n in [(1,'one'),(2,'two'),(3,'three'),(4,'four'),(5,'five')]:\n",
        "        for (i,star_flag,word_flag,num_flag,numpt_flag,num_5_flag,word_5_flag) in tqdm(df[['star_flag','word_'+str(n[0])+'_flag','num_'+str(n[0])+'_flag','num_'+str(n[0])+'pt_flag','num_5_flag','word_5_flag']].itertuples()):\n",
        "            if ((star_flag == 1) and (word_flag == 1)) or ((star_flag == 1) and (word_flag == 1)) or ((star_flag == 1) and (numpt_flag == 1)):\n",
        "                pred = n[0] \n",
        "            else:\n",
        "                pred = 0\n",
        "            predictions[n[0]-1].append(pred)\n",
        "    df['star_max_pred'] = np.array(predictions).max(axis=0)\n",
        "    #df['star_mean_pred'] = np.array(predictions).sum(axis=0) / (np.array(predictions)>0).sum(axis=0)\n",
        "    #print(np.mean((gg.loc[gg.star_max_pred>0,'overall'] - gg.loc[gg.star_max_pred>0,'star_max_pred'])**2))\n",
        "    #print(np.mean((gg.loc[gg.star_mean_pred>0,'overall'] - gg.loc[gg.star_mean_pred>0,'star_mean_pred'])**2)) \n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XREeC_jkwVTj"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8YHkgMaxMk8"
      },
      "source": [
        "# Processing\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "# Metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ-dLbMUhe_f"
      },
      "source": [
        "#X_train, X_test, y_train, y_test = train_test_split(pipeline.df.tokens,pipeline.df.overall,test_size=0.2,random_state=1)\n",
        "X_train = train.tokens.values\n",
        "y_train = train.overall.values\n",
        "X_test = val.tokens.values\n",
        "y_test = val.overall.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLFuabkvxQJA"
      },
      "source": [
        "def dummy_fun(doc):\n",
        "   return doc\n",
        "\n",
        "topk = 3000\n",
        "\n",
        "TF = CountVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer=dummy_fun,\n",
        "    preprocessor=dummy_fun,\n",
        "    token_pattern=None,\n",
        "    max_features=topk\n",
        ")\n",
        "\n",
        "TFIDF = TfidfVectorizer(\n",
        "    analyzer='word',\n",
        "    tokenizer=dummy_fun,\n",
        "    preprocessor=dummy_fun,\n",
        "    token_pattern=None,\n",
        "    max_features=topk\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFQ3DaivxQlS"
      },
      "source": [
        "load_lr_model = False\n",
        "\n",
        "if load_lr_model == True:\n",
        "    lr = pickle.load(open('lr_'+str(topk)+'.p','rb'))\n",
        "    tf = TFIDF.fit(X_train) #TFIDF.fit(train.tokens.values)\n",
        "    x_train = tf.transform(X_train) #tf.transform(train.tokens.values)\n",
        "    #y_train = train.overall\n",
        "    x_val = tf.transform(X_test) #tf.transform(val.tokens.values)\n",
        "    y_val = y_test #val.overall\n",
        "else:\n",
        "    tf = TFIDF.fit(X_train)#TFIDF.fit(train.tokens.values)\n",
        "    x_train = tf.transform(X_train)#tf.transform(train.tokens.values)\n",
        "    #y_train = train.overall\n",
        "    x_val = tf.transform(X_test)#tf.transform(val.tokens.values)\n",
        "    y_val = y_test#val.overall\n",
        "    lr = LogisticRegression(C=0.9,max_iter=5000,random_state=1)\n",
        "    lr.fit(x_train,y_train)\n",
        "    pickle.dump(lr,open('lr_'+str(topk)+'.p','wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MCiIJeeVMLR",
        "outputId": "d1daf3ea-abe0-40c1-aa59-6532f7f6a112"
      },
      "source": [
        "print(np.mean((lr.predict(x_val) - y_val)**2))\n",
        "print(np.mean((lr.predict(x_train) - y_train)**2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7102333333333334\n",
            "0.6390833333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiU1oU5797dV"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r1SDsRCD2xA",
        "outputId": "423792b4-5309-4a72-e4d2-f92dd50f8ac9"
      },
      "source": [
        "# Load val dataframe with RNN preds\n",
        "en = val.copy()\n",
        "# Create LR preds\n",
        "en['lr_pred'] = lr.predict(tf.transform(en.tokens.values)) \n",
        "# Create star flag preds\n",
        "en = star_flags(en)\n",
        "# Filter NULL RNN preds for eval\n",
        "en = en.loc[~en.RNN.isna()].copy()\n",
        "\n",
        "# Not as effective, but we can weight ensemble by number of reviews per user\n",
        "#en['weight'] = (en['n_reviews'] - np.min(en['n_reviews'])) / (np.max(en['n_reviews'])-np.min(en['n_reviews']))\n",
        "#en['pred'] = ((1-en['weight'])*en['lr_pred'] + (en['weight'])*en['RNN'])\n",
        "\n",
        "# Make ensemble predictions\n",
        "error = {}\n",
        "for w1 in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8]:\n",
        "    # Balance between LR and RNN\n",
        "    en['pred_'+str(w1)] = ((1-w1)*en['lr_pred'] + (w1)*en['RNN'])\n",
        "    error[str(w1)] = mean_squared_error(en.overall,en['pred_'+str(w1)])\n",
        "\n",
        "    # If we have a star pred use star pred\n",
        "    en.loc[en.star_max_pred > 0,'pred_'+str(w1)+\"_star\"] = en.loc[en.star_max_pred > 0,'star_max_pred']\n",
        "    en.loc[en.star_max_pred==0,'pred_'+str(w1)+\"_star\"] = en.loc[en.star_max_pred==0,'pred_'+str(w1)]  \n",
        "    \n",
        "    # If we have a star pred, use a version weighting the star pred less than 100%\n",
        "    w2_ = 1 - w1\n",
        "    for w2 in np.arange(0,w2_,0.1)+0.1:\n",
        "        en.loc[en.star_max_pred > 0,'pred_'+str(w1)+\"_\"+str(w2)] = (w1)*en.loc[en.star_max_pred > 0,'RNN'] + (w2_-w2)*en.loc[en.star_max_pred > 0,'lr_pred'] + (w2)*en.loc[en.star_max_pred > 0,'star_max_pred']\n",
        "        en.loc[en.star_max_pred==0,'pred_'+str(w1)+\"_\"+str(w2)] = (w1)*en.loc[en.star_max_pred==0,'RNN'] + (w2_)*en.loc[en.star_max_pred==0,'lr_pred']\n",
        "        if w1 + (w2_-w2) + w2 > 1.01:\n",
        "            print(\"weights exceed 1\")\n",
        "            print((w1,w2_-w2,w2))\n",
        "        error[str(w1)+\"_\"+str(w2)] = mean_squared_error(en.overall,en['pred_'+str(w1)+\"_\"+str(w2)])\n",
        "\n",
        "\n",
        "#en.loc[en.star_flag > 0,'pred_star'] = en.loc[en.star_flag > 0,'star_max_pred']\n",
        "#en.loc[en.star_max_pred==0,'pred_star'] = en.loc[en.star_max_pred==0,'pred_0.6']\n",
        "#print((mean_squared_error(en.overall,en['pred_0.6']),mean_squared_error(en.overall,en['pred_star'])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  return func(self, *args, **kwargs)\n",
            "30000it [00:00, 381799.02it/s]\n",
            "30000it [00:00, 463237.20it/s]\n",
            "30000it [00:00, 435394.88it/s]\n",
            "30000it [00:00, 344428.74it/s]\n",
            "30000it [00:00, 421556.45it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzDgVCdxliu"
      },
      "source": [
        "min_error = pd.DataFrame.from_dict(error,orient='index').sort_values(by=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "jFpCL8chxx7u",
        "outputId": "466a2124-0d9d-44fc-c792-8ef465516165"
      },
      "source": [
        "min_error.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.4_0.5</th>\n",
              "      <td>0.599182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.4_0.6</th>\n",
              "      <td>0.599312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.4_0.4</th>\n",
              "      <td>0.599366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.4_0.30000000000000004</th>\n",
              "      <td>0.599863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.4_0.2</th>\n",
              "      <td>0.600673</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                0\n",
              "0.4_0.5                  0.599182\n",
              "0.4_0.6                  0.599312\n",
              "0.4_0.4                  0.599366\n",
              "0.4_0.30000000000000004  0.599863\n",
              "0.4_0.2                  0.600673"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg2kpEU40zm3",
        "outputId": "5b5d995c-307f-48f3-fb6d-525cb0435d0f"
      },
      "source": [
        "w1 = float(min_error.index[0].split(\"_\")[0])\n",
        "w2_ = 1 - w1\n",
        "w2 = float(min_error.index[0].split(\"_\")[1])\n",
        "print((w1,w2_-w2,w2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0.4, 0.09999999999999998, 0.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSkCbCB6_11I"
      },
      "source": [
        "# Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Aswt-kd_3C4"
      },
      "source": [
        "test = pd.read_csv(\"test.csv\")\n",
        "test_pipeline = Pipeline(test)\n",
        "test_pipeline.run()\n",
        "df_test = test_pipeline.df.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9xrZjXK28WB",
        "outputId": "c3488a50-cb3a-4e8f-d51e-73d39129476c"
      },
      "source": [
        "# Make RNN Pred\n",
        "test_rating_df = test[['user','item']].copy()\n",
        "test_rating_df['RNN'] = rs_model.predict([test_rating_df.user.values,test_rating_df.item.values])\n",
        "# Make Star Flag Pred\n",
        "df_test = star_flags(df_test)\n",
        "# Make LR Pred\n",
        "df_test['lr_pred'] = lr.predict(tf.transform(df_test.tokens.values))\n",
        "# Merge Preds\n",
        "df_test = df_test[['reviewerID','itemID','user','item','star_max_pred','lr_pred']].merge(test_rating_df,how='left',on=['user','item'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/strings.py:2001: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
            "  return func(self, *args, **kwargs)\n",
            "20000it [00:00, 493354.66it/s]\n",
            "20000it [00:00, 440270.61it/s]\n",
            "20000it [00:00, 399975.59it/s]\n",
            "20000it [00:00, 377943.54it/s]\n",
            "20000it [00:00, 403304.28it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3bjGRmAFXnh"
      },
      "source": [
        "df_test.loc[df_test.star_max_pred > 0,'prediction'] = (w1)*df_test.loc[df_test.star_max_pred > 0,'RNN'] + (w2_-w2)*df_test.loc[df_test.star_max_pred > 0,'lr_pred'] + (w2)*df_test.loc[df_test.star_max_pred > 0,'star_max_pred']\n",
        "df_test.loc[df_test.star_max_pred ==0,'prediction'] = (w1)*df_test.loc[df_test.star_max_pred ==0,'RNN'] + (w2_)*df_test.loc[df_test.star_max_pred ==0,'lr_pred']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1jT6ooMFUtT"
      },
      "source": [
        "test = df_test[['reviewerID','itemID','prediction']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "WTNjOKQAc3aG",
        "outputId": "6002301e-38e9-40c9-de3b-b1e0582dd7ce"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>u04140621</td>\n",
              "      <td>p65721979</td>\n",
              "      <td>4.867567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>u74766187</td>\n",
              "      <td>p87809670</td>\n",
              "      <td>4.537337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u31689638</td>\n",
              "      <td>p52702240</td>\n",
              "      <td>4.783937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>u35613516</td>\n",
              "      <td>p01314374</td>\n",
              "      <td>4.223151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>u93320378</td>\n",
              "      <td>p84396269</td>\n",
              "      <td>4.643857</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  reviewerID     itemID  prediction\n",
              "0  u04140621  p65721979    4.867567\n",
              "1  u74766187  p87809670    4.537337\n",
              "2  u31689638  p52702240    4.783937\n",
              "3  u35613516  p01314374    4.223151\n",
              "4  u93320378  p84396269    4.643857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX-UMoMqc63e"
      },
      "source": [
        "test['userID-itemID'] = test['reviewerID']+\"-\"+test['itemID']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuNM3rJwdM6J"
      },
      "source": [
        "rating_pairs = test[['userID-itemID','prediction']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0tnHHNadXql"
      },
      "source": [
        "rating_pairs.to_csv(\"rating_pairs_submission4.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "rHvvx_nGebXZ",
        "outputId": "f6a4f0bc-178d-40ce-dc62-c55f933fc379"
      },
      "source": [
        "rating_pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID-itemID</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>u04140621-p65721979</td>\n",
              "      <td>4.867567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>u74766187-p87809670</td>\n",
              "      <td>4.537337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>u31689638-p52702240</td>\n",
              "      <td>4.783937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>u35613516-p01314374</td>\n",
              "      <td>4.223151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>u93320378-p84396269</td>\n",
              "      <td>4.643857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19995</th>\n",
              "      <td>u16765812-p17291597</td>\n",
              "      <td>4.882435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19996</th>\n",
              "      <td>u04428712-p73630774</td>\n",
              "      <td>4.871598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19997</th>\n",
              "      <td>u39251384-p58275451</td>\n",
              "      <td>4.872159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19998</th>\n",
              "      <td>u05089888-p13155723</td>\n",
              "      <td>4.804947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19999</th>\n",
              "      <td>u99404755-p88280495</td>\n",
              "      <td>4.286469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>20000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             userID-itemID  prediction\n",
              "0      u04140621-p65721979    4.867567\n",
              "1      u74766187-p87809670    4.537337\n",
              "2      u31689638-p52702240    4.783937\n",
              "3      u35613516-p01314374    4.223151\n",
              "4      u93320378-p84396269    4.643857\n",
              "...                    ...         ...\n",
              "19995  u16765812-p17291597    4.882435\n",
              "19996  u04428712-p73630774    4.871598\n",
              "19997  u39251384-p58275451    4.872159\n",
              "19998  u05089888-p13155723    4.804947\n",
              "19999  u99404755-p88280495    4.286469\n",
              "\n",
              "[20000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "muZG2K8OcmZs",
        "outputId": "1e7afe76-15b0-4a00-da7c-3ffd1b4408d1"
      },
      "source": [
        "pd.read_csv('rating_pairs.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-34c4c3e09cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rating_pairs.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rating_pairs.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erlwUv54rYh5"
      },
      "source": [
        "# Archive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1WVW2QbtF4L"
      },
      "source": [
        "### Use vader to evaluated sentiment of reviews\n",
        "def evalSentences(df,cols=['itemID','reviewText','sentiment','score']):\n",
        "    sentences = df['reviewText'].values\n",
        "    review = df[['itemID','reviewText','overall']].itertuples()\n",
        "    #Instantiate an instance to access SentimentIntensityAnalyzer class\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    pd_list = []\n",
        "    for i in review:\n",
        "        item,sentence,score = i[1],i[2],i[3]\n",
        "        ss = sid.polarity_scores(sentence)\n",
        "        pd_list.append((item,sentence,ss['compound'],score))\n",
        "    reviewDf = pd.DataFrame(pd_list)\n",
        "    reviewDf.columns = cols\n",
        "    return reviewDf\n",
        "sentiment = evalSentences(pipeline.df)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(sentiment.score,sentiment.sentiment,alpha=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTpk5GfaO8XA"
      },
      "source": [
        "# Item-Item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK-2thvCO9kT"
      },
      "source": [
        "class SimRecSys(object):\n",
        "    def __init__(self, matrix, num_users, num_items, base, method):\n",
        "        \"\"\"\n",
        "            base: string. From ['user', 'item']. User-based Similarity or Item-based\n",
        "            method: string. From ['cosine', 'euclidean', 'somethingelse']\n",
        "            processor: function name. dataPreprocessor by default\n",
        "        \"\"\"\n",
        "        self.matrix = matrix\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.base = base\n",
        "        self.method_name = method\n",
        "        self.method = self._getMethod(self.method_name)\n",
        "        self.pred_column_name = self.base+'-'+self.method_name\n",
        "    \n",
        "    def _getMethod(self, method_name):\n",
        "        \"\"\"\n",
        "            Don't change this\n",
        "        \"\"\"\n",
        "        switcher = {\n",
        "            'cosine': self.cosine\n",
        "        }\n",
        "        \n",
        "        return switcher[method_name]\n",
        "    \n",
        "    @staticmethod\n",
        "    def cosine(matrix):\n",
        "        \"\"\"\n",
        "            cosine similarity\n",
        "        \"\"\"\n",
        "        similarity_matrix = 1 - pairwise_distances(matrix, metric='cosine')\n",
        "        return similarity_matrix\n",
        "        \n",
        "    def predict_all(self):\n",
        "        train_matrix = self.matrix #self.processor(train_df, num_users, num_items)\n",
        "        temp_matrix = np.zeros(train_matrix.shape)\n",
        "        temp_matrix[train_matrix.nonzero()] = 1\n",
        "        if self.base == 'user':\n",
        "            k = 50\n",
        "            uu_sim = self.method(train_matrix)\n",
        "            m,n = uu_sim.shape\n",
        "            normalizer = np.matmul(uu_sim, temp_matrix)\n",
        "            normalizer[normalizer == 0] = 1e-5\n",
        "            predictionMatrix = np.matmul(uu_sim, train_matrix)/normalizer\n",
        "            # if no one has rated this item before, use user average  \n",
        "            num_user_ratings = np.sum(temp_matrix, axis=1)\n",
        "            num_user_ratings[num_user_ratings == 0] = 1e-5\n",
        "            useraverage = np.sum(train_matrix, axis=1)/num_user_ratings\n",
        "            columns = np.sum(predictionMatrix, axis=0)\n",
        "            predictionMatrix[:, columns==0] = predictionMatrix[:, columns==0] + np.expand_dims(useraverage, axis=1)\n",
        "            self.__model = predictionMatrix\n",
        "            \n",
        "        elif self.base == 'item':\n",
        "            k = 50\n",
        "            # Transpose matrix for item-item similarity scores\n",
        "            uu_sim = self.method(train_matrix.T)\n",
        "            m,n = uu_sim.shape\n",
        "            normalizer = np.matmul(uu_sim, temp_matrix.T)\n",
        "            normalizer[normalizer == 0] = 1e-5\n",
        "            predictionMatrix = np.matmul(uu_sim, train_matrix.T)/normalizer \n",
        "            # if user has no ratings, use item average\n",
        "            num_item_ratings = np.sum(temp_matrix.T, axis=1)\n",
        "            num_item_ratings[num_item_ratings == 0] = 1e-5\n",
        "            itemaverage = np.sum(train_matrix.T,axis=1)/num_item_ratings\n",
        "            columns = np.sum(predictionMatrix, axis=0)\n",
        "            predictionMatrix[:, columns==0] = predictionMatrix[:, columns==0] + np.expand_dims(itemaverage, axis=1)\n",
        "            self.__model = predictionMatrix.T\n",
        "        else:\n",
        "            print('No other option available')\n",
        "    \n",
        "    def reset(self):\n",
        "        try:\n",
        "            self.model = None\n",
        "        except:\n",
        "            print(\"You do not have model..\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_wgA8gAV4Sf"
      },
      "source": [
        "train_pipeline = Pipeline(train)\n",
        "train_pipeline.create_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk59iBT3VrcA"
      },
      "source": [
        "user_cosine_recsys = SimRecSys(train_pipeline.matrix,num_user,num_item,'user','cosine')\n",
        "item_cosine_recsys = SimRecSys(train_pipeline.matrix,num_user,num_item,'item','cosine')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ269s1yWQ0k"
      },
      "source": [
        "#user_cosine_recsys.predict_all()\n",
        "#item_cosine_recsys.predict#_all()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W_M7_bZ7ckE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}